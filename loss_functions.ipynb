{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions in DNN\n",
    "\n",
    "We use the **cross-entropy loss** (or log-loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is based on ideas from information theory. In that context, the **cross entropy** between two probability distributions $p$ and $q$ over the same set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme is optimized for the \"wrong\" distribution $q$, rather than the true distribution, $p$. Basically, it is telling us the average \"message length\" required to identify the underlying distribution is really $p$ even though we assumed it was $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _entropy_ of a distribition is the expected value of the _information_. For a binary classifier, the _Shannon information_ (named after Claude Shannon) is\n",
    "\n",
    "\\begin{equation}\n",
    "I(x_i) = -\\log_2 p \\left(x_i\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The entropy, as an expectation value, is weighted by the probability:\n",
    "\n",
    "\\begin{equation}\n",
    "H = -\\sum_{i} p \\left(x_i\\right) \\log_2 p \\left(x_i\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(pvec):\n",
    "    pvec_ = np.array(pvec)\n",
    "    return np.sum(-pvec_ * np.log2(pvec_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more \"mixed up\" the data is (the more spread across classes), the higher the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.97095059445466858, 0.46899559358928122]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0.5, 0.5]\n",
    "b = [0.4, 0.6]\n",
    "c = [0.1, 0.9]\n",
    "[shannon_entropy(vec) for vec in [a, b, c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3219280948873622, 2.2464393446710154, 1.1219280948873622]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0.2] * 5\n",
    "b = [0.3] + [0.2] * 3 + [0.1]\n",
    "c = [0.8] + [0.05] * 4\n",
    "[shannon_entropy(vec) for vec in [a, b, c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _cross entropy_ is defined according to\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "H\\left(p, q\\right) =&~ \\sum_{x_i} p \\left(x_i\\right) \\log \\frac{1}{q\\left(x_i\\right)} \\\\\n",
    "=&~ - \\sum_{x_i} p \\left(x_i\\right) \\log q\\left(x_i\\right) \n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory can be confusing, but we can recast this in terms of optimization. Consider simple logistic regression for two classes. Here, we model probability with the logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(z\\right) = 1 / \\left(1 + e^{-z}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The probability of finding $y = 1$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{y=1} = \\hat{y} \\equiv g(\\mathbf{w \\cdot x + b})\n",
    "\\end{equation}\n",
    "\n",
    "where, g is our model and $\\mathbf{x}$ is our feature vector and $\\mathbf{w}$ and $\\mathbf{b}$ describe a linear model (which is not a requirement). Similarly, the probability of finding $y = 0$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{y=0} = 1 - \\hat{y}\n",
    "\\end{equation}\n",
    "\n",
    "For the true (observed) probabilities, we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p_{y=1} =&~ y \\\\\n",
    "p_{y=0} =&~ 1 - y \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now, we can write the cross entropy as a similarity measure between p and q:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "H\\left(p, q\\right) =&~ -\\sum_i p_i \\log q_i \\\\\n",
    "=&~ -y \\log \\hat{y} - \\left(1 - y\\right) \\log \\left(1 - \\hat{y}\\right)\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial regression and the softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression uses the logistic function to bound model outputs between zero and one - making them interpretable as probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma \\left(t \\right) = \\frac{1}{1 + e^{-t}}\n",
    "\\end{equation}\n",
    "\n",
    "If we assume $t$ is a linear function of single explanatory variable, we may write it as $t = \\beta_0 + \\beta_1 x$ and the logistic function as\n",
    "\n",
    "\\begin{equation}\n",
    "F(x) = \\frac{1}{1 + e^{- \\left(\\beta_0 + \\beta_1 x \\right)}}\n",
    "\\end{equation}\n",
    "\n",
    "and $F(X)$ is interpreted as the probability of a the dependent variable being a \"success\". We can compute the inverse of the logistic function, or _logit_ as \n",
    "\n",
    "\\begin{equation}\n",
    "g \\left(F \\left(x \\right) \\right) = \\ln \\left(\\frac{F \\left(x \\right)}{F \\left(1 - x \\right)} \\right) = \\beta_0 + \\beta_1 x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor function in multinomial logistic regression is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\ln \\left(\\frac{p_i}{1 - p_i} \\right) =&~ \\beta_{0,k} + \\beta_{1, k} x_{1, i} + \\cdots + \\beta_{M,k} x_{M, i} \\\\\n",
    "=&~ \\mathbf{\\beta_k} \\cdot \\mathbf{x_i} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to derive the multinomial model is to consider for $k$ possible outcomes, running $k-1$ independent binary regression models in which one outcome is chosen as a pivot and the other $k-1$ outocmes are regressed against the pivot outcome. If we choose the last outcome as the pivot, this looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\ln \\frac{Pr \\left(y_i = 1 \\right)}{Pr \\left(y_i = k \\right)} =&~ \\beta_1 \\cdot \\mathbf{x_i} \\\\\n",
    "\\ln \\frac{Pr \\left(y_i = 2 \\right)}{Pr \\left(y_i = k \\right)} =&~ \\beta_2 \\cdot \\mathbf{x_i} \\\\\n",
    "\\cdots&~ \\\\\n",
    "\\ln \\frac{Pr \\left(y_i = k - 1 \\right)}{Pr \\left(y_i = k \\right)} =&~ \\beta_{k - 1} \\cdot \\mathbf{x_i} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "If we exponentiate both sides we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Pr \\left(y_i = 1 \\right) =&~ Pr \\left(y_i = k \\right) e^{\\beta_1 \\cdot \\mathbf{x_i}} \\\\\n",
    "Pr \\left(y_i = 2 \\right) =&~ Pr \\left(y_i = k \\right) e^{\\beta_2 \\cdot \\mathbf{x_i}} \\\\\n",
    "\\cdots&~ \\\\\n",
    "Pr \\left(y_i = k - 1 \\right) =&~ Pr \\left(y_i = k \\right) e^{\\beta_{k - 1} \\cdot \\mathbf{x_i}} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We may then use the fact that all the $k$ probabilities must some to 1:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr \\left(y_i = k \\right) = \\frac{1}{1 + \\sum_{j=1}^{k-1} e^{\\beta_j x_i} }\n",
    "\\end{equation}\n",
    "\n",
    "And so we have for the probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Pr \\left(y_i = 1 \\right) =&~ \\frac{e^{\\beta_1 \\cdot \\mathbf{x_i}}}{1 + \\sum_{j=1}^{k-1} e^{\\beta_j x_i} } \\\\\n",
    "Pr \\left(y_i = 2 \\right) =&~ \\frac{e^{\\beta_2 \\cdot \\mathbf{x_i}}}{1 + \\sum_{j=1}^{k-1} e^{\\beta_j x_i} } \\\\\n",
    "\\cdots&~ \\\\\n",
    "Pr \\left(y_i = k - 1 \\right) =&~ \\frac{e^{\\beta_{k - 1} \\cdot \\mathbf{x_i}}}{1 + \\sum_{j=1}^{k-1} e^{\\beta_j x_i} } \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Or, more generally:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr \\left(y = k \\right) = \\frac{e^{\\beta_k \\cdot \\mathbf{x_i}}}{\\sum_j e^{\\beta_j \\cdot \\mathbf{x_i}}}\n",
    "\\end{equation}\n",
    "\n",
    "This is called the _softmax equation_ and it is common for neural networks to encode their final output using this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when working with a multiclass classifier, we encode the state vector as a \"one-hot\" vector. So, for example if we are identifying an event as coming from target 1, 2, 3, 4, or 5, we write those states as `[1, 0, 0, 0, 0]`, `[0, 1, 0, 0, 0]`, `[0, 0, 1, 0, 0]`, etc.\n",
    "\n",
    "Therefore, we can imagine the final output of a neural network being five neurons that will be interpreted _through a softmax function_ as the probability of being in target 1, 2, 3, 4, or 5. Regardless of what the weights actually are, by passing the final five outputs as a vector to a softmax, we get a number that we may interpret as a probability for each outcome (all are between 0 and 1 and the sum will be 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy can be extended to include this multi-output form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    \"\"\"\n",
    "    note, you wouldn't write a 'real' softmax like this - you need numerical stability tricks\n",
    "    \"\"\"\n",
    "    exp = np.exp(vec)\n",
    "    return exp / exp.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(final_softmax, true_labels):\n",
    "    return np.sum(-true_labels * np.log(final_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0067156544288905244,\n",
       " 2.1269334246813152,\n",
       " 1.6702319748914317e-05,\n",
       " 1.0986122944318208,\n",
       " 1.4076170787652222]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_final_cases = np.array([\n",
    "    [15, 30, 25, 6, 10],    # moderately strong correct reco\n",
    "    [32, 30, 20, 8, 12],    # incorrect\n",
    "    [10, 31, 20, 1, 1],     # strongly correct reco\n",
    "    [30, 30, 30, 12, 10],   # no clear signal\n",
    "    [29, 30, 31, 20, 12],   # weakly incorrect\n",
    "])\n",
    "[cross_entropy(softmax(vec), y_true) for vec in nn_final_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notable properties of this loss function:\n",
    "\n",
    "1. it is bounded from below by zero\n",
    "2. the function goes to zero when the softmax output is close to the true label\n",
    "3. there is no notion of \"distance\" in the \"wrongness\" - off by 1 is as bad as off by 2, etc.\n",
    "\n",
    "Softmax cross entropy is very popular in classification tasks. It is not obviously correct in _localization_ tasks - in that case one might prefer the squared distance (or some other distance metric). We choose to use the softmax cross entropy because the space we are operating in is not linear. That is to say, when mapping a kernel across pixels, the \"distance\" between two columns (two planes) is not the same in every view or even across one view because of the targets and UXVX interleaving, etc.\n",
    "\n",
    "We also tried a distance squared approach when trying to explicitly get `z` as a regression value, but this was not very successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3tf]",
   "language": "python",
   "name": "conda-env-py3tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
